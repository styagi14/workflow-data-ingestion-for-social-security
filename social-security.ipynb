{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2448f22",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import *\n",
    "from pyspark.sql.types import *\n",
    "from delta.tables import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c826260d",
   "metadata": {},
   "outputs": [],
   "source": [
    "storage_account_name = \"social_support_user_details\"\n",
    "container_name = \"input\"\n",
    "mount_point = \"/mnt/datalake\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d80510d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_postgresql_path = f\"{mount_point}/raw/postgresql/users/\"\n",
    "raw_mongodb_path = f\"{mount_point}/raw/mongodb/attachments/\"\n",
    "processed_path = f\"{mount_point}/processed/enriched_user_data/\"\n",
    "analytics_path = f\"{mount_point}/analytics/user_metrics/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86f7d545",
   "metadata": {},
   "outputs": [],
   "source": [
    "configs = {\n",
    "  \"fs.azure.account.auth.type\": \"OAuth\",\n",
    "  \"fs.azure.account.oauth.provider.type\": \"org.apache.hadoop.fs.azurebfs.oauth2.ClientCredsTokenProvider\",\n",
    "  \"fs.azure.account.oauth2.client.id\": dbutils.secrets.get(scope=\"your-scope\", key=\"client-id\"),\n",
    "  \"fs.azure.account.oauth2.client.secret\": dbutils.secrets.get(scope=\"your-scope\", key=\"client-secret\"),\n",
    "  \"fs.azure.account.oauth2.client.endpoint\": f\"https://login.microsoftonline.com/{dbutils.secrets.get(scope='your-scope', key='tenant-id')}/oauth2/token\"\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40f1b8a9",
   "metadata": {},
   "source": [
    "# Read data from postgresql"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "adce7fa0",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_users = spark.read.parquet(raw_postgresql_path)\n",
    "print(f\"Users count: {df_users.count()}\")\n",
    "df_users.printSchema()\n",
    "display(df_users.limit(5))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53d069fa",
   "metadata": {},
   "source": [
    "# Read data from mongodb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8cb871b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_attachments = spark.read.json(raw_mongodb_path)\n",
    "print(f\"Attachments count: {df_attachments.count()}\")\n",
    "df_attachments.printSchema()\n",
    "display(df_attachments.limit(5))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0daa3a8c",
   "metadata": {},
   "source": [
    "## clean and transform user data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c63ab700",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_users_clean = df_users \\\n",
    "    .filter(col(\"user_id\").isNotNull()) \\\n",
    "    .withColumn(\"created_date\", to_date(col(\"created_at\"))) \\\n",
    "    .withColumn(\"full_name\", concat_ws(\" \", col(\"first_name\"), col(\"last_name\"))) \\\n",
    "    .withColumn(\"processed_timestamp\", current_timestamp())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3d17103",
   "metadata": {},
   "source": [
    "## Transform attachment data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "048632a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_attachments_clean = df_attachments \\\n",
    "    .filter(col(\"user_id\").isNotNull()) \\\n",
    "    .withColumn(\"attachment_size_mb\", col(\"size\") / 1024 / 1024) \\\n",
    "    .withColumn(\"file_extension\", \n",
    "                regexp_extract(col(\"filename\"), r'\\.([^.]+)$', 1))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1bcc630",
   "metadata": {},
   "source": [
    "## join user and attachment data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a4d91a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_enriched = df_users_clean.alias(\"u\") \\\n",
    "    .join(\n",
    "        df_attachments_clean.alias(\"a\"),\n",
    "        col(\"u.user_id\") == col(\"a.user_id\"),\n",
    "        \"left\"\n",
    "    ).select(\n",
    "        col(\"u.*\"),\n",
    "        col(\"a.attachment_id\"),\n",
    "        col(\"a.filename\"),\n",
    "        col(\"a.attachment_size_mb\"),\n",
    "        col(\"a.file_extension\"),\n",
    "        col(\"a.upload_date\")\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "462824ee",
   "metadata": {},
   "source": [
    "## Write enriched data as delta table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dde6476e",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_enriched.write \\\n",
    "    .format(\"delta\") \\\n",
    "    .mode(\"overwrite\") \\\n",
    "    .option(\"mergeSchema\", \"true\") \\\n",
    "    .partitionBy(\"created_date\") \\\n",
    "    .save(processed_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "252d2073",
   "metadata": {},
   "source": [
    "## Create Spark SQL table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59a82625",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.sql(f\"\"\"\n",
    "    CREATE TABLE IF NOT EXISTS enriched_user_data\n",
    "    USING DELTA\n",
    "    LOCATION '{processed_path}'\n",
    "\"\"\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Pyspark 3",
   "language": "python",
   "name": "pyspark3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
